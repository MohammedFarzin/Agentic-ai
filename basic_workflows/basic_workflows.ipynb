{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Workflows\n",
    "\n",
    "1. Prompt-Chaining\n",
    "2. Parallelization\n",
    "3. Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict, Callable\n",
    "from util import llm_call, extract_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(input: str, prompts: List[str]) -> str:\n",
    "    \"\"\"Chain multiple LLM calls sequentially, passing results between steps.\"\"\"\n",
    "    result = input\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nStep {i}:\")\n",
    "        result = llm_call(f\"{prompt}\\nInput: {result}\")\n",
    "        print(result)\n",
    "    return result\n",
    "\n",
    "def parallel(prompt: str, inputs: List[str], n_workers: int = 3) -> List[str]:\n",
    "    \"\"\"Process multiple inputs concurrently with the same prompt.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(llm_call, f\"{prompt}\\nInput: {x}\") for x in inputs]\n",
    "        return [f.result() for f in futures]\n",
    "\n",
    "def route(input: str, routes: Dict[str, str]) -> str:\n",
    "    \"\"\"Route input to specialized prompt using content classification.\"\"\"\n",
    "    # First determine appropriate route using LLM with chain-of-thought\n",
    "    print(f\"\\nAvailable routes: {list(routes.keys())}\")\n",
    "    selector_prompt = f\"\"\"\n",
    "    Analyze the input and select the most appropriate support team from these options: {list(routes.keys())}\n",
    "    First explain your reasoning, then provide your selection in this XML format:\n",
    "\n",
    "    <reasoning>\n",
    "    Brief explanation of why this ticket should be routed to a specific team.\n",
    "    Consider key terms, user intent, and urgency level.\n",
    "    </reasoning>\n",
    "\n",
    "    <selection>\n",
    "    The chosen team name\n",
    "    </selection>\n",
    "\n",
    "    Input: {input}\"\"\".strip()\n",
    "    \n",
    "    route_response = llm_call(selector_prompt)\n",
    "    reasoning = extract_xml(route_response, 'reasoning')\n",
    "    route_key = extract_xml(route_response, 'selection').strip().lower()\n",
    "    \n",
    "    print(\"Routing Analysis:\")\n",
    "    print(reasoning)\n",
    "    print(f\"\\nSelected route: {route_key}\")\n",
    "    \n",
    "    # Process input with selected specialized prompt\n",
    "    selected_prompt = routes[route_key]\n",
    "    return llm_call(f\"{selected_prompt}\\nInput: {input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
