{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ChatMessage, SystemMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_completion_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level professional outline of an essay. \\\n",
    "Write such an professional outline for the user provided topic. Give an professional outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays. \\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed:\n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provided detailed recommendations, including requests for length, depth, sytle, funnyness, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outline below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "        \n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = llm.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "\n",
    "    content = state.get('content', [])\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(q, max_results=3)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\n Here is my plan:\\n\\n{state['plan']}\"\n",
    "    )\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = llm.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(q, max_results=3)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7acc152040b0>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7acc152040b0>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7acc152040b0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_conditional_edges(\"generate\", should_continue, {END: END, \"reflect\": \"reflect\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7acc152040b0>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': 'I. Introduction\\n    A. Brief overview of langchain and langsmith\\n    B. Importance of understanding the differences between the two\\n\\nII. Langchain\\n    A. Definition and explanation\\n    B. Key characteristics\\n    C. Use cases and applications\\n\\nIII. Langsmith\\n    A. Definition and explanation\\n    B. Key characteristics\\n    C. Use cases and applications\\n\\nIV. Differences between Langchain and Langsmith\\n    A. Technology\\n    B. Functionality\\n    C. Security\\n    D. Scalability\\n\\nV. Conclusion\\n    A. Recap of key points\\n    B. Importance of choosing the right technology for specific needs\\n    C. Future implications and advancements in the field of language technologies'}}\n",
      "{'research_plan': {'content': ['LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain aids in implementing the retrieval-augmented generation (RAG) pattern in applications, simplifying data retrieval from various sources. LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). Indexes in LangChain serve as databases, organizing and storing information in a structured manner so relevant data is retrieved efficiently when the system processes language queries. LangChain offers a getting started example, to help you create an AI application in just a few minutes. By combining Langflow with LangChain, you accelerate your development process and more easily explore the capabilities of language models in your applications.', 'LangChain is an open source framework for building applications based on large language models (LLMs). For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. LangChain provides AI developers with tools to connect language models with external data sources. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Developers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs. What Is AWS? Developers on AWS', 'Tutorials *LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs).* It provides a standard interface for chains, many integrations with other tools, and end-to-end chains for common applications. LangChain allows AI developers to develop applications based on the combined Large Language Models (such as GPT-4) with external sources of computation and data. LangChain follows a structured pipeline that integrates user queries, data retrieval and response generation into seamless workflow. It provides a framework for connecting language models to other data sources and interacting with various APIs. LangChain is designed to be easy to use, even for developers who are not familiar with lang 13 min read', 'LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications - built for every step of the appli', 'Get started with LangSmith | 🦜️🛠️ LangSmith Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. LangSmith + LangChain OSS LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production. The LangSmith SDK and UI make building and running high-quality evaluations easy. Quickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application. Get started by creating your first prompt.', 'See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith Backend To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. LangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services. LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data). LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback). The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. LangSmith Backend LangSmith SDK']}}\n",
      "{'generate': {'draft': '**Title: LangChain vs LangSmith: Understanding the Differences, Pros, and Cons**\\n\\nI. **Introduction**\\nLangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. Understanding the differences between LangChain and LangSmith is crucial for choosing the right tool for specific needs.\\n\\nII. **LangChain**\\nLangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). It aids in implementing the retrieval-augmented generation (RAG) pattern in applications and offers a structured pipeline for seamless workflow integration. LangChain is ideal for early-stage prototyping and small-scale applications, providing tools for connecting language models with external data sources and creating prompt templates for various applications.\\n\\nIII. **LangSmith**\\nLangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. It offers observability features throughout all stages of application development, from prototyping to production, with tools for prompt engineering and performance evaluation. LangSmith is tailored for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.\\n\\nIV. **Differences between LangChain and LangSmith**\\nA. **Technology**: LangChain is a Python framework focused on AI application development, while LangSmith is a DevOps platform for LLM applications.\\nB. **Functionality**: LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications with advanced debugging and monitoring capabilities.\\nC. **Security**: LangSmith offers LLM-native observability and tools for prompt engineering, enhancing security and performance evaluation.\\nD. **Scalability**: LangChain aids in implementing the RAG pattern and offers a structured pipeline for seamless workflow integration, while LangSmith provides tools for monitoring and debugging large, complex AI systems in production, enhancing scalability.\\n\\nV. **Conclusion**\\nUnderstanding the differences between LangChain and LangSmith is essential for choosing the right technology for specific needs. While LangChain excels at early-stage prototyping and small-scale applications, LangSmith is designed for large-scale, production-ready applications with advanced debugging and monitoring capabilities. The future implications and advancements in the field of language technologies will continue to shape the development and deployment of AI applications.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': \"**Critique:**\\n\\nOverall, your essay provides a clear and concise comparison between LangChain and LangSmith, highlighting their differences in technology, functionality, security, and scalability. The introduction effectively sets the stage for the discussion, and the conclusion nicely summarizes the key points. However, there are areas where you can enhance your essay to make it more engaging and informative.\\n\\n**Recommendations:**\\n\\n1. **Depth and Detail:** Consider expanding on the functionalities of LangChain and LangSmith. Provide specific examples or use cases where each tool excels to give the reader a better understanding of their practical applications.\\n\\n2. **Real-World Examples:** Incorporate real-world examples or case studies to illustrate how LangChain and LangSmith have been used in different scenarios. This will make your essay more relatable and help readers visualize the benefits of each tool.\\n\\n3. **Comparison Metrics:** Include quantitative data or metrics to support your claims about the differences between LangChain and LangSmith. For example, you could discuss performance benchmarks, user adoption rates, or customer testimonials to add credibility to your analysis.\\n\\n4. **Visual Aids:** Consider including visual aids such as charts, graphs, or diagrams to visually represent the key differences between LangChain and LangSmith. Visual elements can help break up the text and make complex information easier to digest.\\n\\n5. **Future Outlook:** Expand on the future implications of using LangChain and LangSmith in the rapidly evolving field of language technologies. Discuss potential advancements, challenges, and opportunities that may impact the development and deployment of AI applications.\\n\\n6. **Engaging Language:** Try to incorporate more engaging language and varied sentence structures to maintain the reader's interest throughout the essay. Consider using anecdotes, analogies, or rhetorical questions to make your points more compelling.\\n\\n7. **Length:** While your essay is informative, consider expanding on each section to provide more in-depth analysis and insights. Aim to strike a balance between brevity and thoroughness to ensure you cover all relevant aspects of the topic.\\n\\nBy incorporating these recommendations, you can enhance the depth, clarity, and engagement of your essay on LangChain vs LangSmith, making it a more comprehensive and compelling read for your audience.\"}}\n",
      "{'research_critique': {'content': ['LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain aids in implementing the retrieval-augmented generation (RAG) pattern in applications, simplifying data retrieval from various sources. LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). LangChain is a Python framework designed to streamline AI application development, focusing on real-time data processing and integration with large language models (LLMs). Indexes in LangChain serve as databases, organizing and storing information in a structured manner so relevant data is retrieved efficiently when the system processes language queries. LangChain offers a getting started example, to help you create an AI application in just a few minutes. By combining Langflow with LangChain, you accelerate your development process and more easily explore the capabilities of language models in your applications.', 'LangChain is an open source framework for building applications based on large language models (LLMs). For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. LangChain provides AI developers with tools to connect language models with external data sources. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Developers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs. What Is AWS? Developers on AWS', 'Tutorials *LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs).* It provides a standard interface for chains, many integrations with other tools, and end-to-end chains for common applications. LangChain allows AI developers to develop applications based on the combined Large Language Models (such as GPT-4) with external sources of computation and data. LangChain follows a structured pipeline that integrates user queries, data retrieval and response generation into seamless workflow. It provides a framework for connecting language models to other data sources and interacting with various APIs. LangChain is designed to be easy to use, even for developers who are not familiar with lang 13 min read', 'LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications - built for every step of the appli', 'Get started with LangSmith | 🦜️🛠️ LangSmith Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. LangSmith + LangChain OSS LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production. The LangSmith SDK and UI make building and running high-quality evaluations easy. Quickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application. Get started by creating your first prompt.', 'See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith Backend To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. LangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services. LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data). LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback). The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. LangSmith Backend LangSmith SDK', 'Each project is presented in a Jupyter notebook and showcases various functionalities such as creating simple chains, using tools, querying CSV files, and interacting with SQL databases. Each project is presented in a Jupyter notebook and showcases various functionalities such as creating simple chains, using tools, querying CSV files, and interacting with SQL databases. Each project is presented in a Jupyter notebook and showcases various functionalities such as creating simple chains, using tools, querying CSV files, and interacting with SQL databases. This project demonstrates how to define custom tools for LangChain and chain them together to perform complex tasks involving CSV files and Wikipedia queries. Each project is presented in a Jupyter notebook and showcases various functionalities such as creating simple chains, using tools, querying CSV files, and interacting with SQL databases.', \"LangChain Examples. In this section, we will show some of the basic functionalities of LangChain with examples so that beginners can understand it better. Throughout the examples. we will work with two LLMs - OpenAI's GPT model and Google's Flan t5 model. This will help us to showcase how easily we can interchange LLMs with the help of\", 'A sample Streamlit web application for summarizing documents using LangChain and Chroma. A sample Streamlit web application for generative question-answering using LangChain, Gemini and Chroma. A sample Streamlit web application to demo LLM observability using LangChain and Helicone. A sample Streamlit application for Google news search and summaries using LangChain and Serper API. A sample Streamlit web application for generative question-answering using LangChain and Pinecone. A sample Streamlit web application for document summarization using LangChain and Pinecone. A sample Streamlit web application for search queries using LangChain and Tavily Search API. A sample Streamlit web application for search queries using LangChain and SerpApi. text-summary A sample Streamlit web application for summarizing text using LangChain and OpenAI.', 'LangSmith Essentials: A Practical Guide to Building LLM Applications. Step into the world of large language models (LLMs) with LangSmith Essentials, your hands-on guide to crafting powerful and practical AI applications for real-world challenges.. Book Summary. Harness the power of LangSmith to design, develop, and deploy cutting-edge applications built on large language models.', 'As a tool, LangSmith empowers you to debug, evaluate, test, and improve your LLM applications continuously. If there\\'s a use-case we missed, or if you have insights to share, please raise a GitHub issue (feel free to tag Will) or contact the LangChain development team. How to download feedback and examples from a test project: goes beyond the utility described above to query and export the predictions, evaluation results, and other information to programmatically add to your reports. Real-time RAG Chat Bot Evaluation: This Streamlit walkthrough showcases an advanced application of the concepts from the Real-time Automated Feedback tutorial. Iterative Prompt Optimization: Streamlit app demonstrating real-time prompt optimization based on user feedback and dialog, leveraging few-shot learning and a separate \"optimizer\" model to dynamically improve a tweet-generating system.', \"Thousands of companies build AI apps better with LangChain products. ### Rakuten Group builds with LangChain and LangSmith to deliver premium products for its business clients ### How Dun & Bradstreet's ChatD&B™ uses LangChain and LangSmith to deliver trusted, data-driven AI insights Not only did we deliver a better product by iterating with LangSmith, but we’re shipping new AI features to our users in a fraction of the time it would have taken without it.” “Working with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn’t have achieved \\xa0the product experience delivered to our customers without LangChain, and we couldn’t have done it at the same pace without LangSmith.”\", 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain Benchmarks 0.0.12 🦜💯 LangChain Benchmarks Benchmark All Tasks Semi-structured eval: Multi vector Evaluating RAG Architectures on Benchmark Tasks Benchmarking Without LangSmith 🦜💯 LangChain Benchmarks Benchmarking Results 🦜💯 LangChain Benchmarks# A package to help benchmark various LLM related tasks. Showing how we collect our benchmark datasets for each task Showing what the benchmark datasets we use for each task is Benchmarking Results# pip install -U langchain-benchmarks The package is located within langchain_benchmarks. Below are archived benchmarks that require cloning this repo to run. Tasks Benchmarking Benchmarking Benchmarking Benchmarking Benchmarking Benchmark All Tasks Define an extraction chain Define an extraction chain Semi-structured eval: Multi vector Evaluating RAG Architectures on Benchmark Tasks Benchmark Tasks and Datasets (As of 2023/11/21) Benchmarking Without LangSmith Benchmarking Results', \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\"]}}\n",
      "{'generate': {'draft': '**Title: LangChain vs LangSmith: Understanding the Differences, Pros, and Cons**\\n\\n**I. Introduction**\\nLangChain and LangSmith are two powerful tools developed by LangChain, aimed at simplifying the building and deployment of Large Language Model (LLM) applications. Understanding the distinctions between these tools is crucial for effective utilization in various scenarios.\\n\\n**II. LangChain**\\nLangChain is a Python framework designed for early-stage prototyping and small-scale applications. It focuses on real-time data processing and integration with large language models (LLMs), offering a structured pipeline for seamless workflow integration.\\n\\n**III. LangSmith**\\nOn the other hand, LangSmith serves as a unified DevOps platform tailored for large-scale, production-ready applications. It provides advanced debugging, testing, and monitoring capabilities essential for managing complex AI systems in production environments.\\n\\n**IV. Differences between LangChain and LangSmith**\\nA. **Technology**: LangChain is more suitable for prototyping and small-scale applications, while LangSmith is geared towards large-scale, production-ready systems.\\nB. **Functionality**: LangChain focuses on streamlining AI application development and data retrieval, whereas LangSmith emphasizes debugging, monitoring, and orchestrating AI and ML models.\\nC. **Security**: LangChain offers tools for connecting language models with external data sources securely, while LangSmith provides advanced debugging features for ensuring the security and stability of AI systems.\\nD. **Scalability**: LangChain aids in implementing the retrieval-augmented generation (RAG) pattern and offers tools for scaling model workflows, while LangSmith excels in managing large-scale workflows with multiple moving parts.\\n\\n**V. Conclusion**\\nIn conclusion, choosing between LangChain and LangSmith depends on the specific requirements of the project at hand. Understanding the differences in technology, functionality, security, and scalability is crucial for selecting the right tool for the job. As advancements in language technologies continue, the importance of leveraging the appropriate tools for AI development will only grow, shaping the future of AI applications.', 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for s in graph.stream({\n",
    "        \"task\": \"What is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ewriter' from 'helper' (/home/farzin/AI/Agentic-ai/env/lib/python3.12/site-packages/helper/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhelper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ewriter, writer_gui\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ewriter' from 'helper' (/home/farzin/AI/Agentic-ai/env/lib/python3.12/site-packages/helper/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting helper\n",
      "  Downloading helper-2.5.0.tar.gz (18 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/farzin/AI/Agentic-ai/env/lib/python3.12/site-packages (from helper) (6.0.2)\n",
      "Building wheels for collected packages: helper\n",
      "  Building wheel for helper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for helper: filename=helper-2.5.0-py2.py3-none-any.whl size=19220 sha256=f59ca188e48e730acdf743e0d7c8935b329e82a5782fb2fe0874aca504558518\n",
      "  Stored in directory: /home/farzin/.cache/pip/wheels/7a/58/02/457999369b4271ecfffaa3d7a67d4501e3ca8ae97bca785ab8\n",
      "Successfully built helper\n",
      "Installing collected packages: helper\n",
      "Successfully installed helper-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
